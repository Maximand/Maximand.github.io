<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-05-30T22:35:55+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Disclosing.Observer</title><subtitle>Test
</subtitle><author><name>Max</name><email>mhvanderhorst@tudelft.nl</email></author><entry><title type="html">Ready, Retain, Fire? The Quiet Fallout of U.S. Offensive Cyber Policy</title><link href="http://localhost:4000/2025/05/30/us-cyber-policy-zero-day-retention.html" rel="alternate" type="text/html" title="Ready, Retain, Fire? The Quiet Fallout of U.S. Offensive Cyber Policy" /><published>2025-05-30T00:00:00+02:00</published><updated>2025-05-30T00:00:00+02:00</updated><id>http://localhost:4000/2025/05/30/us-cyber-policy-zero-day-retention</id><content type="html" xml:base="http://localhost:4000/2025/05/30/us-cyber-policy-zero-day-retention.html"><![CDATA[<p>Being a CNA administrator means having access to something most people never see: zero-day vulnerabilities. That access comes with responsibility, but also perspective. It’s made me think about how other actors, especially governments, handle this kind of knowledge, and what it means when one side holds a stockpile of vulnerabilities capable of large-scale surveillance or disruption. The U.S. military even <a href="https://cyber.army.mil/News/Article/1325442/stockpiling-zero-day-exploits-the-next-international-weapons-taboo/">considered</a> classifying zero-days as weapons subject to export controls.</p>

<p>I was encouraged to see that some governments have public procedures designed to manage this power transparently. But lately, it feels like that transparency is under pressure.</p>

<p>When the United Kingdom launched the HMS <em>Dreadnought</em> in 1906, it redefined naval warfare. The dreadnought was a warship so advanced, that it wasn’t just a display of power: it was a strategic signal that reshaped expectations. Germany responded by expanding its fleet, triggering a naval arms race rooted less in intent and more in perceived necessity.</p>

<h1 id="the-zero-day-dreadnought">The zero-day dreadnought</h1>

<figure class="figure-float-right">
  <img style="padding:10px; max-width:79%;" src="../../../assets/VEP_Process.png" alt="VEP Process" />
  <figcaption>Figure 1: The VEP Flowchart,<br /> taken from trumpwhitehouse.archives.gov</figcaption>
</figure>

<p>Today, a similar pattern seems to be emerging in the digital realm. As the Trump administration returned in 2025, its cybersecurity strategy is shifting from structured deterrence to a more aggressive, <a href="https://therecord.media/trump-administration-change-the-script-on-offensive-hacking">offense-first approach</a>, aligning with Trump’s <em>Peace Through Strength</em> doctrine. This posture <a href="https://www.iiss.org/cyber-power-matrix/anticipating-trumps-influence-on-us-cyber-command/">builds on the foundation laid by National Security Presidential Memoranda 13 (NSPM-13)</a> in 2018, which loosened interagency controls over offensive cyber operations.</p>

<p>One important, but seemingly overlooked, mechanism to the U.S. offensive cyberpower is the <a href="https://trumpwhitehouse.archives.gov/sites/whitehouse.gov/files/images/External%20-%20Unclassified%20VEP%20Charter%20FINAL.PDF">Vulnerabilities Equities Process (VEP)</a>, which is the Government Disclosure Decision Process (GDDP) used to decide whether to disclose or retain newly discovered zero-day vulnerabilities. While the VEP itself has not formally changed under Trump’s current or previous administrations, it involves significant discretion, particularly in assessing whether disclosure would harm national security. Within a policy environment that is increasingly shaped by offense, there is a growing risk that this discretion may increasingly tilt toward retention.</p>

<p>Though no explicit mandate has been issued, the combination of centralizing authority, reducing transparency, and <a href="https://www.whitehouse.gov/wp-content/uploads/2025/05/Fiscal-Year-2026-Discretionary-Budget-Request.pdf">dismantling oversight mechanisms</a> sends a clear signal to other states: retention is not an exception, it may be becoming the norm. Like the dreadnought, this strategic posture may reset international expectations–not through declaration, but by example.</p>

<h1 id="from-deterrence-to-retention">From deterrence to retention?</h1>
<p>One of the effects of the NSPM-13 was that it streamlined the approval process for offensive cyber operations. This facilitated the <em>Defend Forward</em> doctrine, enabling U.S. Cyber Command to preemptively disrupt threats without explicit presidential approval. While proponents argue this <a href="https://www.cybercom.mil/Media/News/Article/3198878/cyber-101-defend-forward-and-persistent-engagement/">enhances deterrence</a>, research from the Atlantic Council suggests an <a href="https://www.atlanticcouncil.org/programs/cyber-statecraft-initiative/the-proliferation-of-offensive-cyber-capabilities/">overreliance on offensive cyber operations</a> can increase global instability, as adversaries mirror these tactics and <a href="https://www.cato.org/policy-analysis/myth-cyber-offense-case-restraint?#cyber-command-s-new-more-aggressive-policy">see this as provocation</a> instead of deterrence.</p>

<p>The Biden administration adjusted this trajectory by introducing additional checks and transparency mechanisms. However, the first half of 2025 suggests that the second Trump administration may reverse this course. The weakening of oversight mechanisms, such as <a href="https://www.documentcloud.org/documents/25500093-dhs-advisory-boards-termination-letter/">the Cyber Safety Review Board (CSRB)</a>, and the consolidation of decision-making authority within the executive branch indicate a growing emphasis on offensive capability.</p>

<p>This evolution doesn’t come with an explicit declaration that the U.S. is retaining more vulnerabilities. However, the surrounding policy signals–reduced transparency, diminished oversight, and intensified offensive posture–suggest a trend that warrants scrutiny.</p>

<h1 id="strategic-signaling-and-the-dreadnought-effect">Strategic signaling and the dreadnought effect</h1>
<p>Just like the dreadnought reset the bar for naval power, a more opaque and offense-first U.S. cyber doctrine may risk shifting global norms. Vulnerability retention, which was once carefully weighed through the VEP, may increasingly be seen as the default by other nations watching U.S. behavior. This matters because it sends signals, not only to adversaries, but also to allies. Only a few other countries have a GDDP: the <a href="https://www.gchq.gov.uk/information/equities-process">United Kingdom</a> and <a href="https://www.aivd.nl/onderwerpen/onbekende-kwetsbaarheden#:~:text=Kwetsbaarheid%20melden%2C%20tenzij…&amp;text=Hiervoor%20geldt%20het%20beleid%3A%20'melden,(voorlopig)%20niet%20te%20melden.">the Netherlands</a> have a defined GDDP, and Germany has explored a GDDP since 2018, but has not implemented a formal process to date. Because GDDPs are self-imposed constraints, most countries seem to have not prioritized their development or public debate. If the U.S. treats vulnerabilities as strategic assets rather than shared risks, other governments may feel compelled to follow its lead. Even those working to strengthen Coordinated Vulnerability Disclosure (CVD), such as the European Union through the Cyber Resilience Act and the NIS2 Directive, could face internal pressure to reassess their approach. In the end, a GDDP is a way for states to impose accountability on themselves. Without such a process, states may retain greater flexibility, but at the cost of reduced transparency.</p>

<blockquote>
  <p>The <strong>dreadnought effect</strong> refers to the unintended consequences of a dominant power’s strategic innovation resetting international norms. Named after HMS Dreadnought, a revolutionary British battleship launched in 1906, the term describes how a single state’s advancement can pressure others to escalate—even if they had no initial desire to do so. In cybersecurity, the metaphor applies to how shifts in U.S. offensive posture may lead other nations to change their own vulnerability disclosure strategies, simply to maintain parity.</p>
</blockquote>

<h1 id="the-narrow-corridor-and-the-red-queen-effect">The narrow corridor and the red queen effect</h1>
<p><a href="https://www.penguinrandomhouse.com/books/555400/the-narrow-corridor-by-daron-acemoglu-and-james-a-robinson/">Acemoglu and Robinson’s <em>Narrow Corridor</em></a> frames healthy governance as a balance between state power and societal oversight. In this model, both must evolve jointly, providing equal pressure to one another. If one accelerates while the other stagnates, the system risks slipping into authoritarian overreach or institutional weakness.</p>

<p>A GDDP shaped by secrecy, unchecked executive power, and offensive priorities can threaten that balance. The VEP has already <a href="https://www.zetter-zeroday.com/u-s-government-disclosed-39-zero-day-vulnerabilities-in-2023-per-first-ever-report/">faced criticism</a> for a perceived lack of transparency, as reflected in the <a href="https://www.wyden.senate.gov/imo/media/doc/fy23_unclassified_vep_annual_reportpdf.pdf">ODNI’s first public disclosure report</a> on retained and disclosed vulnerabilities. The criticism highlights the need for greater public insight into the VEP to determine whether it genuinely prioritizes defense over offense, something recent developments appear to contradict. The public is losing insight into how vulnerabilities are handled, while the private sector–which is often on the frontlines of cyberattacks–is left in the dark. This lack of transparency further erodes accountability, which again reinforces a shift towards cyber power centralization within the executive branch.</p>

<p>In the case of the United States, this trajectory is a textbook example of something larger. The Narrow Corridor describes the concept of a <em>Despotic Leviathan</em>(originating from Hobbes’ <em>Leviathan</em>), where unchecked state control weakens security and liberty. With the removal of oversight mechanisms and consolidation of cybersecurity authority, the Trump administration risks prioritizing short-term strategic advantages over long-term resilience, ultimately weakening U.S. cybersecurity. This concern is particularly important in light of the current <a href="https://www.politico.com/news/magazine/2025/01/28/trump-tiktok-bailout-00200800">congressional inaction</a> and the <a href="https://time.com/7210420/justice-department-fires-employees-prosecutions-trump/">removal of government officials</a> involved in Trump’s criminal prosecution, further undermining institutional checks and balances.</p>

<p>This is where the <em>Red Queen</em> concept from the Narrow Corridor becomes relevant. To stay secure, both the state and society must run to remain in the “narrow corridor”. By actively sidelining society’s oversight role, the Trump administration disrupts this balance, undermining the reciprocal strengthening that supports resilient governance, robust cybersecurity, and civil liberty.</p>

<h1 id="consequences-and-global-shifts">Consequences and global shifts</h1>
<ul>
  <li><strong>Allies may feel compelled to retain:</strong> U.S. posture may inadvertently pressure other countries to deprioritize disclosure, even if it contradicts their public policy.</li>
  <li><strong>Private sector is left exposed:</strong> Coordinating large-scale vulnerability disclosures has shown me just how far the ripples from a single flaw can reach. When retained vulnerabilities leak (or worse, when no one knows they exist), the fallout isn’t theoretical. A great example is EternalBlue: originally retained by the NSA, it was later leaked and exploited in the WannaCry and NotPetya attacks, which were two of the most disruptive cybersecurity incidents in recent history. These attacks weren’t launched by state actors, but by criminal and proxy groups.</li>
  <li><strong>Norms begin to fracture:</strong> The idea of disclosure as responsible security weakens when strategic ambiguity dominates global cyber policy.</li>
</ul>

<h1 id="conclusion-rediscovering-the-corridor">Conclusion: rediscovering the corridor</h1>
<p>Zero-day vulnerabilities are not just technical artifacts, they are instruments of policy. While the U.S. has not explicitly stated it is retaining more vulnerabilities, its broader cyber strategy sends signals that may shift global expectations.</p>

<p>Like the dreadnought before World War I, offensive cyber capability can reset norms in dangerous ways. To remain in the Narrow Corridor, the U.S. should lead by example: strengthening oversight, maintaining transparency, and upholding vulnerability disclosure as a pillar of cyber resilience.</p>

<p>From where I stand, real strength doesn’t lie in how many vulnerabilities are kept, but in what is disclosed, when, and why. That’s the foundation of trust–and trust, <a href="https://archive.org/details/kerckhoffs.translated.pdf">not secrecy</a>, is what makes the internet more secure.</p>]]></content><author><name>Max van der Horst</name><email>mhvanderhorst@tudelft.nl</email></author><category term="Vulnerability Disclosure" /><category term="Cybersecurity Policy &amp; Ethics" /><summary type="html"><![CDATA[When one nation hoards weapons, others feel compelled to follow. The U.S. posture on zero-day retention risks global insecurity through a dynamic we've seen before.]]></summary></entry><entry><title type="html">What You Hide Will Hurt You: The Streisand Effect of Zero-Day Vulnerabilities</title><link href="http://localhost:4000/2025/05/24/what-you-hide-will-hurt-you-zero-day-vulnerabilities.html" rel="alternate" type="text/html" title="What You Hide Will Hurt You: The Streisand Effect of Zero-Day Vulnerabilities" /><published>2025-05-24T00:00:00+02:00</published><updated>2025-05-24T00:00:00+02:00</updated><id>http://localhost:4000/2025/05/24/what-you-hide-will-hurt-you-zero-day-vulnerabilities</id><content type="html" xml:base="http://localhost:4000/2025/05/24/what-you-hide-will-hurt-you-zero-day-vulnerabilities.html"><![CDATA[<p>When I first became a CVE Numbering Authority (CNA) administrator at DIVD, I assumed most software vendors would welcome a heads-up about critical vulnerabilities in their products. We’re not selling zero-days, we’re helping these vendors remediate them. However, over time, I learned a strange truth: many vendors would rather silence the messenger than fix the message. From legal threats to ostrich politics, the instinct to cover-up a zero-day vulnerability runs deep. Ironically, it’s this reaction (rather than the vulnerability itself) that often causes the most reputational damage. What you hide will hurt you. And in cybersecurity, few things backfire harder than trying to bury insecurity.</p>

<h1 id="life-as-a-cna-administrator">Life as a CNA administrator</h1>
<p>CNAs are organizations authorized by the <a href="https://cve.mitre.org">CVE Program</a> to assign CVE IDs to newly discovered vulnerabilities and publish related details. CNAs are the backbone of the distributed vulnerability reporting system that powers CVE. They can be software vendors, coordination centers, bug bounty platforms, or, like DIVD, security research groups.</p>

<p>Being a CNA administrator at DIVD means acting as a trusted intermediary in the vulnerability disclosure process. Specifically, DIVD is designated as a Research CNA: a type of CNA authorized to assign CVE IDs for vulnerabilities discovered through its own independent security research. Unlike vendor CNAs, which typically cover the security of their own products, Research CNAs do not need to be the creator or maintainer of the affected software and do not rely on vendor permission to assign a CVE. As a Research CNA administrator, we receive vulnerability reports, validate them, coordinate with the vendors where possible, and publish CVE records when Coordinated Vulnerability Disclosure processes have followed through. This role is about much more than just managing identifiers though, it’s about navigating technical, ethical, and sometimes political terrain.</p>

<p>At DIVD, our approach is public-interest driven: typically we are of the opinion that the public has the right to know about a security issue in the software they use. After all, it may affect them personally and they have the right to decide whether or not they want to continue using this software. Therefore, after making sure we have successfully coordinated a disclosure and a remediation has been issued, we also tend to seek out people that are still affected by the vulnerability in order to warn them. The first step in doing this is always to coordinate a disclosure with the vendor, otherwise we end up resorting to disproportionate methodology as is <a href="https://disclosing.observer/2025/05/10/unsolicited-vulnerability-disclosure-ethics.html">extensively discussed in my article on disclosure ethics</a>. Our mission is clear, but the path isn’t always smooth. Some vendors cooperate openly, understanding the value of transparency and early warnings. Others treat our contact as a threat instead of a courtesy, despite the fact that we’re offering voluntary help, not blame.</p>

<p>Over time, this role teaches you something about organizational maturity, communication under pressure, and how reputations are built (or broken) in the way vendors respond to bad news.</p>

<p><img src="../../../assets/CNA-workflow.svg" alt="" class="centered-img" />
<span class="centered-text">Figure 1: CNA workflow</span></p>

<h1 id="the-playbook-of-avoidance">The playbook of avoidance</h1>
<p>Not all companies react the same way to a vulnerability disclosure. Some respond with a clear acknowledgement, quick triage, and a patch timeline. Others will stall, deny, escalate to legal teams, or straight up ignore you hoping to demoralize so that you give up. All of this happens before anyone technical has even looked at the report. These tactics aren’t just frustrating, they’re risky. In the case of some critical vulnerabilities, the time to patch needs to be as short as possible. Every delay increases the window of exploitation and, paradoxically, draws more attention when the issue finally surfaces.</p>

<p>Earlier this year we handled a report that shows just how long some vendors will try to wait you out. An independent researcher, now a member of DIVD, had uncovered a set of high-severity vulnerabilities in a software product widely used by government agencies in one particular country. He had been emailing the vendor, politely and persistently, since 2022. He had received no response. Once the report reached DIVD we re-validated the bugs and started our standard outreach: attempts to contact by phone, email, and LinkedIn. Three months passed, after which we requested that this country’s national CSIRT relay the warning. They, too, were met with radio silence. With 90 days gone and no progress, we published a limited disclosure that included the newly assigned CVE IDs and the advice to not use this product anymore due to the fundamental design weaknesses. Within twenty-four hours, the vendor surfaced: not with a remediation plan, but with a complaint that they “did not want their customers to know about these issues” and that they “wanted the publication taken down”. This is a perfect example of the <em>ignore-until-it-hurts</em> tactic: three years of quiet, private nudging produced nothing; one short public notice produced an immediate, though still defensive, response.</p>

<p>This is why we tend to alert vendors that we are sticking to <a href="https://csirt.divd.nl/cna/">our own written CNA procedure</a>. This procedure is based on the <a href="https://english.ncsc.nl/publications/publications/2019/juni/01/coordinated-vulnerability-disclosure-the-guideline">Coordinated Vulnerability Disclosure guideline</a> by the Dutch National Cyber Security Center (NCSC-NL). This guideline discusses the general consensus that a 60-day window should be sufficient to fix most software vulnerabilities. Hardware vulnerabilities should take longer and have a 180-day window. Of course, as long as a vendor is communicating openly about their progress, we will not be stringent about this. However, we are not afraid to use this as a tool to leverage a response and ultimately inform the public about the (at that point still unpatched) vulnerability. We will do this through a so-called limited disclosure and product warning. Vendors get to deliver input on the framing of any publications as we do not move towards this type of last resort, making it clearly in their best interest to collaborate and take responsibility. In the end, limited disclosures that contain the information that a vendor was not available for response or remediation do not look good.</p>

<h1 id="the-role-of-the-security-industry-in-this-mess">The role of the security industry in this mess</h1>
<p>But let’s be honest: part of the problem may be the cybersecurity industry as a whole. We have created some of the very incentives that make vendors want to hide their flaws. For years, we’ve treated the number of CVEs associated with a vendor as a scoreboard for vulnerability. Many practitioners still do. Think report headlines like <em>Top 10 Most Vulnerable Vendors This Year</em>. What does a high CVE count say to the public? Poor security. Broken systems. Incompetence.</p>

<p>In reality, it’s often the opposite. A high number of CVEs can signal maturity of the vulnerability management processes within an organization: people are looking, problems are being fixed, and the system is transparent enough to document it. That’s a sign of a functioning security posture rather than a failing one. For example, in 2024, the <a href="https://www.cvedetails.com/top-50-vendors.php?year=2024">top vendor with distinct vulnerabilities registered as CVEs </a> was <em>Linux</em> with 3874 CVE IDs. The Linux community is likely one of the most security-aware ones out there, which is partially why they keep finding new issues that are then registered as CVEs. When we shame companies for having vulnerabilities, which is something that happens to the best of us, we create incentives for censorship. That’s on us. If disclosure is to work, we have to stop treating visibility as guilt. At this point, one could even say that it is suspicious when a vendor does not have any claimed CVE IDs or otherwise published vulnerabilities.</p>

<h1 id="when-the-cover-up-becomes-the-story">When the cover-up becomes the story</h1>
<p>Here’s the thing about unresolved limited disclosures: most vulnerabilities don’t become front-page news on their own. What gets remembered, and what is reported, is how a company handles the situation once it becomes aware of the problem. In many cases, the vulnerability itself isn’t what harms a company’s reputation. It’s the response.</p>

<p>Obstructing disclosure, delaying patches, trying to silence or ignore researchers. It creates a second story: one that says they don’t take security seriously. That second story spreads faster and sticks longer. Public shaming often follows obstruction, in contrast to the belief of some of these vendors that having to publish CVE IDs will tarnish their reputation. Media and researchers take note of silence and hostility. Customers, partners, and regulators may start asking questions. The issue inevitably becomes public, but now with a damaging narrative: lack of transparency and accountability. A great example of this is <a href="https://www.cnbc.com/2020/04/02/zoom-ceo-apologizes-for-security-issues-users-spike-to-200-million.html">the repeated security issues with videoconferencing software Zoom</a>, in which the vendor was aware of various vulnerabilities that were discovered by the public. Because of the vulnerabilities, the publicity it received, and the (lack of) response by the company behind Zoom, various large companies like NASA and SpaceX <a href="https://www.zdnet.com/article/zoom-were-freezing-all-new-features-to-sort-out-security-and-privacy/">banned the use of Zoom altogether</a>, setting an important precedent.</p>

<p>That’s the Zero-Day Streisand effect. The more you try to hide the problem, the more visible and damaging it becomes.</p>

<h1 id="what-good-looks-like">What good looks like</h1>
<p>Far from all responses are defensive or hostile. There are plenty of companies I’ve worked with that handle disclosures with transparency, speed, and professionalism. The contrast is absolutely striking. These are the organizations that take action within hours, thank us for the report, and keep us in the loop on remediation progress. They prioritize patching the issue, even when it’s inconvenient. Some even take it a step further by publishing advisories, calling customers, publicly crediting the researcher, and using the opportunity to show their commitment to security.</p>

<p>These companies don’t just fix bugs, they build trust. Ironically, some of the best reputations I’ve seen were forged in the heat of a serious vulnerability. Because when you handle bad news well, it says something powerful in my opinion: you’re not perfect, but you’re responsible. Companies that respond constructively earn real credibility, even when the vulnerability is a serious one. A result of this is that potential PR hits are turned into a trust-building moment both internally and externally. They signal that they take their security seriously, and this is what their customers (and employees) remember.</p>

<h1 id="conclusion-transparency-is-the-real-fix">Conclusion: transparency is the real fix</h1>
<p>Every product has vulnerabilities. That’s not the issue. The issue is whether you fix them, or spend your valuable time fighting the people that try to help you do so. Trying to hide zero-day vulnerabilities doesn’t protect a brand. It fractures trust. Once the cover-up becomes the story, you’ve lost control of the narrative. The Streisand Effect takes over.</p>

<p>Security isn’t about being flawless. It’s about being accountable. We need to stop punishing companies for being transparent, and this is an upcoming trend I am very happy to see. We need to recognize and reward taking the right steps when things become difficult. This is because in the long run, what you hide will hurt you.</p>]]></content><author><name>Max van der Horst</name><email>mhvanderhorst@tudelft.nl</email></author><category term="Vulnerability Disclosure" /><summary type="html"><![CDATA[Most vulnerabilities never make headlines; botched disclosures do. Trying to muzzle researchers doesn’t shrink risk, it spotlights it.]]></summary></entry><entry><title type="html">The Ransomware Blame Game: Who Bears the Burden of Sanction Enforcement?</title><link href="http://localhost:4000/2025/05/17/ransomware-blame-game.html" rel="alternate" type="text/html" title="The Ransomware Blame Game: Who Bears the Burden of Sanction Enforcement?" /><published>2025-05-17T00:00:00+02:00</published><updated>2025-05-17T00:00:00+02:00</updated><id>http://localhost:4000/2025/05/17/ransomware-blame-game</id><content type="html" xml:base="http://localhost:4000/2025/05/17/ransomware-blame-game.html"><![CDATA[<p>When I worked in a Computer Emergency Response Team (CERT), ransomware cases were part of the routine. A company would be hit, backups failed, and the question of ransom payment would come up. Every so often, the team would offer the option of a sanction checking service to verify whether payment was even legal. However, these sanction checks would depend on indicators like cryptocurrency wallet addresses, file hashes, IP addresses, and domain names. These were all indicators that were known to be volatile, so I wondered: <em>how effective is this really?</em>.</p>

<p>These checks felt necessary but rarely brought clarity. Infrastructure changed constantly. Attribution was fragile and full of assumptions, and the same group might be known under a different name next week. Even when the style of a ransomware operation was recognized, it didn’t guarantee that we could connect it to a known actor. The attacker would disappear after the ransom payment, but the legal and financial risk remained, now shifted onto the victim.</p>

<p>That discomfort stuck with me. So when I had the opportunity to start working on <a href="https://www.usenix.org/conference/usenixsecurity25/presentation/van-der-horst">the paper this post is based on</a>, it wasn’t just about the fragility of different Indicators of Compromise (IoCs). 	It was about something deeper: what happens when policy assumes a level of certainty that defenders simply don’t have? This blogpost is an extension of the USENIX paper: a reflection on how our current enforcement and compliance frameworks place responsibility on those with the least access to reliable information. It is not a call to weaken sanctions policy, but a call to make it more just. If sanctions are meant to constrain attackers, then enforcement must be designed to reach them–not to retroactively penalize the victims.</p>

<h1 id="the-limits-of-technical-attribution">The limits of technical attribution</h1>
<p>In Threat Intelligence, attribution is a layered process. <em>Low-level IoCs</em>, pieces of evidence related to infrastructure, the attack, payment methods, and other operational details, are easy to collect and useful for detection. However, they are extremely volatile. These indicators can be changed with little effort by attackers, making them poor foundations for long-term attributions. This idea has been confirmed by frameworks like Bianco’s <a href="https://detect-respond.blogspot.com/2013/03/the-pyramid-of-pain.html">Pyramid of Pain</a> and Rid and Buchanan’s <a href="https://cs.brown.edu/courses/cs180/sources/Attributing_Cyber_Attacks.pdf">Q-Model</a>.</p>

<p>Hence, it seems irresponsible to base sanctions lists on these low-level indicators, which is why the paper behind this post investigates the value of <em>high-level IoCs</em>. By contrast, high-level IoCs capture behavioral traits such as how attackers deploy their malware, how they move laterally, ransom note linguistics, negotiation attitudes, and Tactics, Techniques, and Procedures (TTPs). Both models agree that high-level indicators are more resilient to evasion and thus more trustworthy across rebrandings.</p>

<p><img src="../../../assets/models.png" alt="" class="centered-img" />
<span class="centered-text">Figures 1 &amp; 2: The Pyramid of Pain and the Q-Model</span></p>

<h2 id="the-affiliate-wild-card">The affiliate wild-card</h2>
<p>Many ransomware operations follow an affiliate model known as Ransomware-as-a-Service (RaaS), in which core developers lease their tools to independent partners. These affiliates vary widely in skill and methods, and while operators often provide deployment guidelines, affiliates may diverge from them in practice. As a result, high-level indicators, such as lateral movement techniques or tooling preferences, can differ significantly even within the same group, adding noise to attribution efforts.</p>

<h2 id="observations-from-the-paper">Observations from the paper</h2>
<p>We analysed datasets containing information on ransomware incidents: 27 private incident reports from an incident response company and 13 public CISA advisories based on various other incident response organizations. Two findings stood out from this data:</p>

<figure class="figure-float-right">
  <img src="../../../assets/radar-chart-overlap-sim.png" alt="Radar chart" />
  <figcaption>Figure 3: Similarity metrics across ransomware groups</figcaption>
</figure>

<ul>
  <li><strong>Inside a single “brand,” the technical overlap was surprisingly low.</strong> Different incidents linked to the same group often shared fewer than half their TTPs.</li>
  <li><strong>Across sources, the overlaps became even lower.</strong> What CISA published and what the responders from our partnering company observed for the same group only line up part of the time. This shows a significant discrepancy between the reporting of different organizations, which undermines the assumption that defenders (or regulators) have access to a consistent view of attacker behaviour at the time sanctions checks are performed.</li>
</ul>

<p>In short, the low-level indicators that sanctions lists currently lean on tend to expire quickly, while no one seems to have a comprehensive overview of the behavioural high-level indicators. While this dataset is of course not exhaustive, the consistency of these patterns across both public and private reporting streams underscores their broader relevance.</p>

<h2 id="why-this-matters-for-sanctions">Why this matters for sanctions</h2>
<p>Yet, most compliance frameworks, and even some insurance policies, continue to rely on indicators that appear to uniquely identify a ransomware group. This is an understandable position: enforcement systems require clear identifiers to function. However, this expectation favors indicators that are easily assigned over indicators that are more stable but less individually distinctive.</p>

<p>As a result, and as confirmed by our interviewees, high-level behavioural indicators are often excluded from compliance checks, not because they are unreliable, but because they lack the singularity needed to support legal or financial action. Sanctions enforcement, therefore, depends on low-level indicators not due to their robustness, but because they are uniquely attributable. This is true even when those same indicators can be changed by attackers with minimal effort.</p>

<p>Until enforcement frameworks find a way to operationalise high-level signals across rebrandings, defenders will have to deal with indicators that are known to change rapidly. In practice, this means working with the most accessible signals, not the most meaningful ones.</p>

<h1 id="rebranding-as-a-sanctions-evasion-strategy">Rebranding as a sanctions-evasion strategy</h1>
<p>Ransomware groups don’t stay still. After public exposure, <a href="https://www.cisecurity.org/insights/blog/the-conti-leaks-a-case-of-cybercrimes-commercialization">internal conflict</a>, or sanctions, many simply dissolve and rebrand under a new name. The infamous Conti group, for instance, splintered into <a href="https://www.hhs.gov/sites/default/files/blacksuit-ransomware-analyst-note-tlpclear.pdf">several new entities</a>, including Royal and then BlackSuit, among others. These new groups often adopt new infrastructure, names, leak sites, and negotiation portals, but retain (some of) the same personnel, malware toolchains, and affiliate relationships.</p>

<p>This rebranding breaks the link between old and new entities in legal and public intelligence contexts. While threat researchers may recognize the continuity, governments may typically require formal evidence of control or ownership to link a new group to a sanctioned predecessor. That process is slow, often opaque, and rarely public. Months can pass before rebranded groups are re-sanctioned, during which victims remain legally exposed.</p>

<p>Victims, meanwhile, are expected to know better. If they pay a ransom to a group that is later tied to a sanctioned actor, they may be penalized, even if the link was not publicly known at the time of payment. Various interviewees in the study mentioned this to be a great risk to the victim, as insurers can (and will) try to claw back a paid-out ransom months later if the recipient is sanctioned after the fact, effectively retro-dating the exclusion to the payment date. Moreover, the moment a ransomware group is sanctioned, it may trigger a rebrand, further obfuscating the public overview. This is the heart of the policy asymmetry: ransomware groups can easily obscure their identities whereas defenders and victims are expected to see through the smoke.</p>

<h1 id="the-information-deficit-and-the-burden-of-compliance">The information deficit and the burden of compliance</h1>
<p>This means that the position that a victim of ransomware finds themselves in can be framed as an information deficit. The entities responsible for enforcing compliance, such as governments, regulators, and insurers, operate on the assumption that attribution is clear and actionable. However, in ransomware cases, attribution is often incomplete, delayed, or ambiguous.</p>

<p>The ransomware cases we’ve analyzed in our study show that low-level indicators rarely persisted across incidents, let alone rebrandings. In several cases, ransom payments were made to groups not publicly sanctioned at the time, only for links to sanctioned actors to emerge through later intelligence analysis. These links often relied on high-level behavior indicators like linguistics, panel design reuse, or TTPs, which were absent from standard sanction checks.</p>

<p>This creates a <em>critical gap</em>: compliance decisions are expected to be based on indicators that are technically available but operationally unreliable. Meanwhile, indicators that could offer stronger attribution are often not included in threat feeds or sanctions designations, and are not legally recognized as sufficient evidence. Furthermore, because of phenomena like the Ransomware-as-a-Service (RaaS) ecosystem, slight deviations in reporting using frameworks like the <a href="https://attack.mitre.org">MITRE ATT&amp;CK Matrix</a>, and the commercialisation of the Threat Intelligence industry, data is so fragmented that no single organization appears to have a comprehensive view of high-level indicators, making coordinated enforcement even more difficult. This also means that even when TTP combinations appear distinctive on paper, the fragmentation and inconsistency across sources make them impractical for real-world sanctions checks.</p>

<p>In practice, this means that defenders may act in good faith based on the best available data and still find themselves in violation of compliance expectations. While European countries are typically more forgiving than the U.S. by taking into account their <a href="https://finance.ec.europa.eu/document/download/65560de8-a13a-4a58-a87c-ddd27b14e6c1_en?filename=faqs-sanctions-russia-best-efforts-obligation_en.pdf"><em>Best Efforts regulation</em></a>, payment to a sanctioned entity still opens up a victim to legal or financial consequences. 	Several European and American cyber-insurance wordings now not only <a href="https://assets.lloyds.com/media/47dd9b48-e881-4169-8a98-8d305a1d6fce/Y5359.pdf">exclude reimbursement for ransoms that benefit a sanctioned party</a>, but also reserve the right to <a href="https://cilj.law.uconn.edu/wp-content/uploads/sites/2520/2022/04/Ransomware-Kenneally-CILJ-Vol.-28.1.pdf"><strong>recover a payout after the fact</strong></a> if new intelligence or a late-issued designation reveals that the attackers were sanctioned. Retroactive enforcement of these exclusions, whether via contract or regulation, shifts the risk back onto those least equipped to verify attribution in real time: the victims.</p>

<h1 id="structural-asymmetry-in-ransomware-enforcement">Structural asymmetry in ransomware enforcement</h1>
<p>The outcome is a deeply asymmetrical system. Attackers operate in a fluid, pseudonymous ecosystem. Defenders operate under legal obligations, audit trails, and policy scrutiny. Compliance rules are often applied rigidly. Not only by governments, but increasingly by cyber insurers, even when the available indicators are ambiguous.</p>

<p>Our research calls attention to this structural misalignment. It is not simply a technical issue, but a policy failure. By placing the burden of accurate attribution on defenders, we ignore the reality that attribution is contested and delayed, even among experts.</p>

<p>As I mentioned in the beginning of this article, this post does not argue against the principle of sanctions. It can be a legitimate tool of pressure, deprivation, and deterrence. Rather, it argues that sanctions policy must evolve to strike at the actors it was meant to constrain, not those it was meant to protect. Enforcement that punishes victims through uncertainty does not advance justice; it undermines it.</p>

<h1 id="policy-recommendations-toward-risk-aware-compliance">Policy recommendations: toward risk-aware compliance</h1>
<p>Of course, I am not arguing that we should abandon sanctions policy altogether. Instead, it would be better to propose a more realistic and risk-aware approach:</p>

<ul>
  <li><strong>Recognize the limits of low-level indicators and require corroboration.</strong> Enforcement actions should not hinge on volatile indicators alone. Additional behavioral or contextual evidence should be required to support sanctions-related decisions.</li>
  <li><strong>Promote the development and sharing of behavioral-level CTI.</strong> Encourage public-private sharing of ransomware tradecraft indicators and support open databases that track rebrand linkages, like the U.S. Cybersecurity and Infrastructure Security Agency’s (CISA) <a href="https://www.cisa.gov/stopransomware">#StopRansomware campaign</a>, where they share observed indicators from various organizations.</li>
  <li><strong>Establish a cross-sector CTI clearing house.</strong> All 20 experts we interviewed independently called for a neutral, non-commercial platform to consolidate and curate ransomware attribution data. No single entity today has the necessary scope or trust to do this alone.</li>
  <li><strong>Introduce proportionality and good-faith standards</strong> in compliance enforcement. If a victim acted on available information and had no access to classified or high-confidence attribution, they should be protected instead of penalized anywhere.</li>
  <li><strong>Improve transparency of sanctions intelligence.</strong> Where governments establish control or continuity between groups, this information should be shared with defenders, not just for enforcement.</li>
</ul>

<p>These are not radical departures from current policy. They are adjustments grounded in the technical and operational realities of the ransomware ecosystem.</p>

<h1 id="conclusion-enforcement-should-follow-evidence-not-assumption">Conclusion: enforcement should follow evidence, not assumption</h1>
<p>Sanctions are a legitimate tool of deterrence. But in the ransomware context, their enforcement often assumes more clarity and certainty than defenders and victims actually possess. Attribution is rarely unambiguous. Indicators change, groups rebrand, and intelligence arrives too late to guide real-time decisions. Current enforcement frameworks place responsibility on victims to act with <em>perfect foresight</em> in an environment defined by uncertainty. This doesn’t advance justice—it undermines it.</p>

<p>If policy is to support resilience rather than amplify harm, it must reflect how threat intelligence actually works. That means supporting enforcement with persistent, behavioral signals rather than fragile, low-level ones. It also means ensuring that all parties involved have access to the same information. It means recognizing the asymmetries between attackers who can vanish and reappear at will, and defenders who are left to navigate compliance obligations in the dark.</p>

<p>We can’t sanction what we can’t see. And we shouldn’t punish those who were never the intended target.</p>]]></content><author><name>Max van der Horst</name><email>mhvanderhorst@tudelft.nl</email></author><category term="Cybersecurity Policy &amp; Ethics" /><category term="Observations on Cyber Threat Intelligence" /><category term="Research Process" /><summary type="html"><![CDATA[When I worked in a Computer Emergency Response Team (CERT), ransomware cases were part of the routine. A company would be hit, backups failed, and the question of ransom payment would come up. Every so often, the team would offer the option of a sanction checking service to verify whether payment was even legal. However, these sanction checks would depend on indicators like cryptocurrency wallet addresses, file hashes, IP addresses, and domain names. These were all indicators that were known to be volatile, so I wondered: how effective is this really?. These checks felt necessary but rarely brought clarity. Infrastructure changed constantly. Attribution was fragile and full of assumptions, and the same group might be known under a different name next week. Even when the style of a ransomware operation was recognized, it didn’t guarantee that we could connect it to a known actor. The attacker would disappear after the ransom payment, but the legal and financial risk remained, now shifted onto the victim. That discomfort stuck with me. So when I had the opportunity to start working on the paper this post is based on, it wasn’t just about the fragility of different Indicators of Compromise (IoCs). It was about something deeper: what happens when policy assumes a level of certainty that defenders simply don’t have? This blogpost is an extension of the USENIX paper: a reflection on how our current enforcement and compliance frameworks place responsibility on those with the least access to reliable information. It is not a call to weaken sanctions policy, but a call to make it more just. If sanctions are meant to constrain attackers, then enforcement must be designed to reach them–not to retroactively penalize the victims. The limits of technical attribution In Threat Intelligence, attribution is a layered process. Low-level IoCs, pieces of evidence related to infrastructure, the attack, payment methods, and other operational details, are easy to collect and useful for detection. However, they are extremely volatile. These indicators can be changed with little effort by attackers, making them poor foundations for long-term attributions. This idea has been confirmed by frameworks like Bianco’s Pyramid of Pain and Rid and Buchanan’s Q-Model. Hence, it seems irresponsible to base sanctions lists on these low-level indicators, which is why the paper behind this post investigates the value of high-level IoCs. By contrast, high-level IoCs capture behavioral traits such as how attackers deploy their malware, how they move laterally, ransom note linguistics, negotiation attitudes, and Tactics, Techniques, and Procedures (TTPs). Both models agree that high-level indicators are more resilient to evasion and thus more trustworthy across rebrandings. Figures 1 &amp; 2: The Pyramid of Pain and the Q-Model The affiliate wild-card Many ransomware operations follow an affiliate model known as Ransomware-as-a-Service (RaaS), in which core developers lease their tools to independent partners. These affiliates vary widely in skill and methods, and while operators often provide deployment guidelines, affiliates may diverge from them in practice. As a result, high-level indicators, such as lateral movement techniques or tooling preferences, can differ significantly even within the same group, adding noise to attribution efforts. Observations from the paper We analysed datasets containing information on ransomware incidents: 27 private incident reports from an incident response company and 13 public CISA advisories based on various other incident response organizations. Two findings stood out from this data: Figure 3: Similarity metrics across ransomware groups Inside a single “brand,” the technical overlap was surprisingly low. Different incidents linked to the same group often shared fewer than half their TTPs. Across sources, the overlaps became even lower. What CISA published and what the responders from our partnering company observed for the same group only line up part of the time. This shows a significant discrepancy between the reporting of different organizations, which undermines the assumption that defenders (or regulators) have access to a consistent view of attacker behaviour at the time sanctions checks are performed. In short, the low-level indicators that sanctions lists currently lean on tend to expire quickly, while no one seems to have a comprehensive overview of the behavioural high-level indicators. While this dataset is of course not exhaustive, the consistency of these patterns across both public and private reporting streams underscores their broader relevance. Why this matters for sanctions Yet, most compliance frameworks, and even some insurance policies, continue to rely on indicators that appear to uniquely identify a ransomware group. This is an understandable position: enforcement systems require clear identifiers to function. However, this expectation favors indicators that are easily assigned over indicators that are more stable but less individually distinctive. As a result, and as confirmed by our interviewees, high-level behavioural indicators are often excluded from compliance checks, not because they are unreliable, but because they lack the singularity needed to support legal or financial action. Sanctions enforcement, therefore, depends on low-level indicators not due to their robustness, but because they are uniquely attributable. This is true even when those same indicators can be changed by attackers with minimal effort. Until enforcement frameworks find a way to operationalise high-level signals across rebrandings, defenders will have to deal with indicators that are known to change rapidly. In practice, this means working with the most accessible signals, not the most meaningful ones. Rebranding as a sanctions-evasion strategy Ransomware groups don’t stay still. After public exposure, internal conflict, or sanctions, many simply dissolve and rebrand under a new name. The infamous Conti group, for instance, splintered into several new entities, including Royal and then BlackSuit, among others. These new groups often adopt new infrastructure, names, leak sites, and negotiation portals, but retain (some of) the same personnel, malware toolchains, and affiliate relationships. This rebranding breaks the link between old and new entities in legal and public intelligence contexts. While threat researchers may recognize the continuity, governments may typically require formal evidence of control or ownership to link a new group to a sanctioned predecessor. That process is slow, often opaque, and rarely public. Months can pass before rebranded groups are re-sanctioned, during which victims remain legally exposed. Victims, meanwhile, are expected to know better. If they pay a ransom to a group that is later tied to a sanctioned actor, they may be penalized, even if the link was not publicly known at the time of payment. Various interviewees in the study mentioned this to be a great risk to the victim, as insurers can (and will) try to claw back a paid-out ransom months later if the recipient is sanctioned after the fact, effectively retro-dating the exclusion to the payment date. Moreover, the moment a ransomware group is sanctioned, it may trigger a rebrand, further obfuscating the public overview. This is the heart of the policy asymmetry: ransomware groups can easily obscure their identities whereas defenders and victims are expected to see through the smoke. The information deficit and the burden of compliance This means that the position that a victim of ransomware finds themselves in can be framed as an information deficit. The entities responsible for enforcing compliance, such as governments, regulators, and insurers, operate on the assumption that attribution is clear and actionable. However, in ransomware cases, attribution is often incomplete, delayed, or ambiguous. The ransomware cases we’ve analyzed in our study show that low-level indicators rarely persisted across incidents, let alone rebrandings. In several cases, ransom payments were made to groups not publicly sanctioned at the time, only for links to sanctioned actors to emerge through later intelligence analysis. These links often relied on high-level behavior indicators like linguistics, panel design reuse, or TTPs, which were absent from standard sanction checks. This creates a critical gap: compliance decisions are expected to be based on indicators that are technically available but operationally unreliable. Meanwhile, indicators that could offer stronger attribution are often not included in threat feeds or sanctions designations, and are not legally recognized as sufficient evidence. Furthermore, because of phenomena like the Ransomware-as-a-Service (RaaS) ecosystem, slight deviations in reporting using frameworks like the MITRE ATT&amp;CK Matrix, and the commercialisation of the Threat Intelligence industry, data is so fragmented that no single organization appears to have a comprehensive view of high-level indicators, making coordinated enforcement even more difficult. This also means that even when TTP combinations appear distinctive on paper, the fragmentation and inconsistency across sources make them impractical for real-world sanctions checks. In practice, this means that defenders may act in good faith based on the best available data and still find themselves in violation of compliance expectations. While European countries are typically more forgiving than the U.S. by taking into account their Best Efforts regulation, payment to a sanctioned entity still opens up a victim to legal or financial consequences. Several European and American cyber-insurance wordings now not only exclude reimbursement for ransoms that benefit a sanctioned party, but also reserve the right to recover a payout after the fact if new intelligence or a late-issued designation reveals that the attackers were sanctioned. Retroactive enforcement of these exclusions, whether via contract or regulation, shifts the risk back onto those least equipped to verify attribution in real time: the victims. Structural asymmetry in ransomware enforcement The outcome is a deeply asymmetrical system. Attackers operate in a fluid, pseudonymous ecosystem. Defenders operate under legal obligations, audit trails, and policy scrutiny. Compliance rules are often applied rigidly. Not only by governments, but increasingly by cyber insurers, even when the available indicators are ambiguous. Our research calls attention to this structural misalignment. It is not simply a technical issue, but a policy failure. By placing the burden of accurate attribution on defenders, we ignore the reality that attribution is contested and delayed, even among experts. As I mentioned in the beginning of this article, this post does not argue against the principle of sanctions. It can be a legitimate tool of pressure, deprivation, and deterrence. Rather, it argues that sanctions policy must evolve to strike at the actors it was meant to constrain, not those it was meant to protect. Enforcement that punishes victims through uncertainty does not advance justice; it undermines it. Policy recommendations: toward risk-aware compliance Of course, I am not arguing that we should abandon sanctions policy altogether. Instead, it would be better to propose a more realistic and risk-aware approach: Recognize the limits of low-level indicators and require corroboration. Enforcement actions should not hinge on volatile indicators alone. Additional behavioral or contextual evidence should be required to support sanctions-related decisions. Promote the development and sharing of behavioral-level CTI. Encourage public-private sharing of ransomware tradecraft indicators and support open databases that track rebrand linkages, like the U.S. Cybersecurity and Infrastructure Security Agency’s (CISA) #StopRansomware campaign, where they share observed indicators from various organizations. Establish a cross-sector CTI clearing house. All 20 experts we interviewed independently called for a neutral, non-commercial platform to consolidate and curate ransomware attribution data. No single entity today has the necessary scope or trust to do this alone. Introduce proportionality and good-faith standards in compliance enforcement. If a victim acted on available information and had no access to classified or high-confidence attribution, they should be protected instead of penalized anywhere. Improve transparency of sanctions intelligence. Where governments establish control or continuity between groups, this information should be shared with defenders, not just for enforcement. These are not radical departures from current policy. They are adjustments grounded in the technical and operational realities of the ransomware ecosystem. Conclusion: enforcement should follow evidence, not assumption Sanctions are a legitimate tool of deterrence. But in the ransomware context, their enforcement often assumes more clarity and certainty than defenders and victims actually possess. Attribution is rarely unambiguous. Indicators change, groups rebrand, and intelligence arrives too late to guide real-time decisions. Current enforcement frameworks place responsibility on victims to act with perfect foresight in an environment defined by uncertainty. This doesn’t advance justice—it undermines it. If policy is to support resilience rather than amplify harm, it must reflect how threat intelligence actually works. That means supporting enforcement with persistent, behavioral signals rather than fragile, low-level ones. It also means ensuring that all parties involved have access to the same information. It means recognizing the asymmetries between attackers who can vanish and reappear at will, and defenders who are left to navigate compliance obligations in the dark. We can’t sanction what we can’t see. And we shouldn’t punish those who were never the intended target.]]></summary></entry><entry><title type="html">Unsolicited but Ethical: Threshold Deontology in Public Interest Vulnerability Disclosure</title><link href="http://localhost:4000/2025/05/10/unsolicited-vulnerability-disclosure-ethics.html" rel="alternate" type="text/html" title="Unsolicited but Ethical: Threshold Deontology in Public Interest Vulnerability Disclosure" /><published>2025-05-10T00:00:00+02:00</published><updated>2025-05-10T00:00:00+02:00</updated><id>http://localhost:4000/2025/05/10/unsolicited-vulnerability-disclosure-ethics</id><content type="html" xml:base="http://localhost:4000/2025/05/10/unsolicited-vulnerability-disclosure-ethics.html"><![CDATA[<p>I often send emails to people I’ve never met, about systems they didn’t know were vulnerable, warning them about risks they never asked me to find. Often, they’re surprised. Mostly grateful. Occasionally hostile.</p>

<p>I can understand the discomfort. On the surface, without more in-depth knowledge, it can feel intrusive. Who asked me to scan their infrastructure? Who gave me permission to notify them about something they didn’t request? I’ve come to realize that in cybersecurity, waiting for consent isn’t always an ethical luxury we can afford. Sometimes, when a vulnerability threatens publicly accessible systems, the consequences of inaction quickly outweigh the social comfort of protocol.</p>

<h1 id="ethics-of-vulnerability-disclosure">Ethics of Vulnerability Disclosure</h1>
<p>At the <a href="https://divd.nl">Dutch Institute for Vulnerability Disclosure</a>, I work in a team that operates in that uncomfortable space between respecting boundaries and preventing harm. It’s not a line we walk casually, and one I’ve learned to approach with both caution and conviction. That’s why, at DIVD, we rely on a shared <a href="https://divd.nl/code">Code of Conduct</a> (CoC). This CoC is derived from a <a href="https://www.om.nl/documenten/richtlijnen/2020/december/14/om-beleidsbrief-ethisch-hacken#:~:text=Het%20Openbaar%20Ministerie%20heeft%20in,tegen%20een%20ethische%20hacker%20binnenkomt.">policy</a> that was published by the Dutch Public Prosecutor in 2020, which describes the circumstances under which computer hacking will be considered ethical and exempt from prosecution. This policy is based on years of debate and legal jurisprudence and has served the Dutch hacker community greatly up until now. The questions it provides to determine whether or not an action is ethical are as follows:</p>

<ul>
  <li>Was the action taken in the context of a significant public interest?</li>
  <li>Was the conduct proportionate (i.e., did the suspect not go further than necessary to achieve their objective)?</li>
  <li>Was the requirement of subsidiarity met (i.e., were there no less intrusive means available to achieve the intended objective)?</li>
</ul>

<p>The DIVD CoC guides our decisions to make sure we do not cross any ethical boundaries. This can get knotty quite quickly, as we attempt to locate everyone worldwide that may be vulnerable to a particular security vulnerability. Therefore we have to make well-thought-through decisions when it comes to worldwide vulnerability scans: we have to be sure that we do not intrude further than necessary and that our way of scanning is the least impactful one.</p>

<p>I’ve found myself to be increasingly exacting about this. If a scan doesn’t meet the ethical standards we’ve committed to at DIVD, it’s not uncommon to argue against executing it–even if that means walking away from a serious case. That may seem cautious, but it raises a deeper ethical question: When is a vulnerability severe enough that inaction becomes the more problematic choice?</p>

<p>This post argues that the ethical frameworks implicitly used by DIVD, particularly threshold deontology, provide a defensible basis for more intrusive forms of unsolicited vulnerability disclosure when the public interest is at stake.</p>

<h1 id="when-inaction-becomes-the-problem">When inaction becomes the problem</h1>
<p>Ethics in the computer security landscape have always been a topic of discussion. Recently, this discussion seems to have picked up in the academic world around the three leading frameworks and their respective ideologies:</p>

<ul>
  <li><strong>Consequentialism:</strong> Actions are morally right if it leads to the best overall outcomes or consequences. The use of consequentialism in this article mostly resembles utilitarianism, which is a type of consequentialism that focuses on the well-being of people.</li>
  <li><strong>Deontology:</strong> Actions are morally right if it follows a set of moral rules or duties, regardless of the outcome.</li>
  <li><strong>Virtue Ethics:</strong> Actions are morally right if it reflects the character and virtues of a good or morally exemplary person.</li>
</ul>

<p>While equally important, this post does not focus on virtue ethics. The challenge we face in Coordinated Vulnerability Disclosure is not about judging moral character. It’s about operationalizing structured processes, making ethically defensible decisions under pressure, and balancing duties with consequences. These are domains where rules and outcomes matter more directly than personal virtue.</p>

<p>This is confirmed by various studies in the academic landscape such as <a href="https://www.dhs.gov/sites/default/files/publications/CSD-MenloPrinciplesCORE-20120803_1.pdf">The Menlo Report</a> and the (more recent) study on <a href="https://www.usenix.org/system/files/usenixsecurity23-kohno.pdf">Computer Security Trolley Problems</a> by Kohno et al. These studies emphasize the importance of consequentialism and deontology where they intentionally leave virtue ethics out of scope. In contrast, studies of cybercrime — where intent, personal responsibility, and moral development are central — often lean more heavily on virtue ethics.</p>

<p>The tension between doing what’s right according to principle and doing what’s necessary to prevent harm lies at the center of many dilemmas in computer security. Deontology and consequentialism are often seen as opposites that lead to different outcomes when applied to the same case studies, leading these case studies to be seen as moral dilemmas. For this reason, one may realise that an absolutist approach to either of these frameworks may not be sufficient in practice when the intent is preventing harm. The Stanford Encyclopedia of Philosophy emphasizes this limitation by describing a balance through what is known as <a href="https://plato.stanford.edu/entries/ethics-deontological/#DeoRelConRec">threshold deontology</a>. Threshold deontology begins with a commitment to deontological principles such as minimizing intrusion and acting transparently. However, it recognizes that these rules may need to be overridden when the potential harm of inaction crosses a critical threshold. In other words:</p>

<blockquote>
  <p>We follow the rules, until not following them becomes the more ethical choice.</p>
</blockquote>

<p>Threshold deontology doesn’t abandon principles. It does, however, ask us to honor them until the consequences of strict adherence become morally unacceptable, to then act with caution for a societal cause.</p>

<h1 id="principles-in-practice">Principles in practice</h1>
<p>But how would we know when that threshold is actually crossed? Threshold deontology provides us with the ‘philosophical permission’ to override a duty, but it doesn’t say when exactly that override is justified. This is where the principlist framework can provide some guidance. Instead of relying on a single guiding rule, it asks us to weigh multiple ethical principles that often come into tension in practice. This helps us to assess not just whether an action is justified, but also why and what ethical trade-offs we are accepting in the process.</p>

<p>In 2021, Formosa et al. proposed <a href="https://www.sciencedirect.com/science/article/pii/S0167404821002066">a principlist framework</a> for cybersecurity that is composed of the following five principles:</p>

<ul>
  <li><strong>Beneficence:</strong> Promote well-being and prevent harm</li>
  <li><strong>Non-maleficence</strong>: Avoid causing harm</li>
  <li><strong>Autonomy:</strong> Respect individuals’ control over their systems and data</li>
  <li><strong>Justice:</strong> Ensure fairness and equitable treatment</li>
  <li><strong>Explicability:</strong> Act transparently and be accountable</li>
</ul>

<p>Formosa’s principlist framework is derived from Beauchamp and Childress’s <a href="https://jme.bmj.com/content/28/5/332.2">“Four Principles” of biomedical ethics</a> and added a fifth principle of explicability, which is drawn from <a href="https://link.springer.com/article/10.1007/s11023-018-9482-5">AI ethics</a> (Floridi et al., 2018). When we’re considering something like a global vulnerability scan, these principles help us structure our ethical reasoning. DIVD’s scanning decisions are centered on the principle of beneficence: the obligation to prevent harm and promote public safety. When the potential benefit of scanning is low, for example when a vulnerability poses little risk or is unlikely to be exploited, we hold firm to the non-maleficence (avoiding harm) and autonomy (respecting consent and responsibility) principles. In such cases, we will refrain from scanning because the ethical cost outweighs the limited benefit.</p>

<p>However, when the potential to prevent significant harm is high, such as when a vulnerability threatens large-scale exploitation or critical infrastructure, we may override non-maleficence and autonomy in service of that benefit. This is not a decision taken lightly. It reflects a careful ethical trade-off, where the duty to protect others justifies limited, well-controlled intrusion.</p>

<h1 id="how-wide-is-my-fingerprint">How WIDE is my fingerprint?</h1>
<p>To teach others about fingerprinting ethics, I often use a simple heuristic to assess the moral footprint of a fingerprinting technique called WIDE. WIDE stands for:</p>

<ul>
  <li><strong>Weaponized:</strong> Could this scanning methodology be used to (enable) harm? Or more practical: if the scanning methodology involves a public Proof of Concept, does it contain any malware that we would need to neutralize first? This reflects the principle of non-maleficence.</li>
  <li><strong>Intrusive:</strong> Does this scanning methodology cross any boundaries of consent, privacy, or proportionality? Does it leave any unnecessary traces on the target system? This brings autonomy and justice into view.</li>
  <li><strong>Deweaponized:</strong> Is this scanning methodology deliberately designed to reduce its exploitability? This ties to beneficence, the duty to protect.</li>
  <li><strong>Ethical:</strong> Would this technique hold up under scrutiny from others? Is it proportionate and according to subsidiarity standards? Here, explicability becomes essential to ensure transparency on decisions and reasoning.</li>
</ul>

<p><img src="../../../assets/WIDE_heuristic_diagram_darkbg.svg" alt="" /></p>

<p>When I ask myself, <em>“How WIDE is this fingerprint?”</em>, I’m not answering a closed question. I’m surfacing tensions. Even techniques that appear technically harmless can become ethically problematic if used carelessly, at scale, or without transparency. WIDE isn’t a substitute for principlism, but it does help bring the principles into everyday practice. It’s a kind of ethical gut check for ethical proportionality: quick, imperfect, but useful when decisions happen fast. Of course, ethics are subjective, which is why not everyone may agree with our framing. Especially when consent is missing, ethical objections matter.</p>

<h1 id="addressing-ethical-objections">Addressing ethical objections</h1>
<p>There are some <a href="https://www.hup.harvard.edu/books/9780674976009">counterarguments to unsolicited scanning</a>, such as concerns about overreach, digital trespassing, and the potential erosion of trust in security research. In the end, if subjectivity leads to the justification of compromising on the explicability principle, ethical boundaries are allowed to shift under pressure. How do we ensure they don’t shift too far?</p>

<p>These are legitimate concerns. However, this is exactly why it is important to rely on structured frameworks like threshold deontology and principlism: not to escape ethical boundaries, but to make these boundaries visible, contestable, and constrained. The point isn’t that anything is allowed when societal safety is at play. It’s that sometimes, doing nothing carries a greater ethical cost than acting carefully without permission.</p>

<h1 id="the-public-interest-threshold-of-log4shell">The public interest threshold of Log4Shell</h1>
<p>When <a href="https://www.cisa.gov/news-events/news/apache-log4j-vulnerability-guidance">Log4Shell</a> was disclosed in late 2021, it posed a severe threat to global digital infrastructure. The vulnerability affected countless systems, many of which were unknown to be using this software, and exploitation began within hours of public disclosure as it was trivial to exploit. It became clear quickly that this vulnerability didn’t just pose a theoretical risk, but a real-world crisis.</p>

<p>Log4J, the vulnerable software, was a logging component embedded in many other systems, making this a supply chain issue. The team at DIVD working on this case then faced a difficult question: how to responsibly notify affected organizations across the globe, many of which had no idea they were even using Log4J?</p>

<p>In such a high-stakes context, the threshold for ethical intervention was clearly crossed. The potential for harm was immense, as this vulnerability could lead to ransomware, data breaches, and critical infrastructure failures, disrupting society on a large scale. Because Log4J was embedded in other software rather than a standalone component, scanning was not possible without triggering the vulnerability itself. The considerations central to the WIDE heuristic helped to assess this approach: where many actors were mass-exploiting systems at random to test for the vulnerability, DIVD’s scanning methodology was deliberately deweaponized to reduce exploitability and kept as non-intrusive as possible by avoiding persistence or harmful effects.</p>

<p>To achieve this, DIVD created a Log4Shell exploit that triggered a single DNS request to a <a href="https://www.canarytokens.org/nest/">Canary Token</a> from inside the vulnerable system: an approach that was harmless and, of all options, the least intrusive. It only revealed the vulnerable IP address and nothing more. The actions taken were designed to be ethical: proportionate to the risk, grounded in subsidiarity, and subject to public scrutiny. In doing so, we aimed to maximize beneficence, minimize maleficence, respect justice, and uphold explicability—even where autonomy had to be limited for the sake of public safety.</p>

<h1 id="conclusion-trust-through-transparent-ethics">Conclusion: Trust through transparent ethics</h1>
<p>Coordinated Vulnerability Disclosure isn’t just a technical challenge: it’s an ethical one. At DIVD, we don’t treat unsolicited scanning and disclosure as a loophole or an afterthought. We treat it as an action that requires justification, restraint, and transparency.</p>

<p>Threshold deontology provides us with the ethical architecture to act decisively when society is at risk. The principlist framework implicitly helps us navigate that threshold with clarity, so we’re not acting on instinct, but on structured ethical reasoning. I’m sharing this to explain how I think about the ethics behind unsolicited disclosure. Not because we see ourselves as above ethical rules at DIVD, but because we try to follow them as rigorously as possible, even when the path forward isn’t obvious.</p>]]></content><author><name>Max van der Horst</name><email>mhvanderhorst@tudelft.nl</email></author><category term="Cybersecurity Policy &amp; Ethics" /><category term="Vulnerability Disclosure" /><summary type="html"><![CDATA[I often send emails to people I’ve never met, about systems they didn’t know were vulnerable, warning them about risks they never asked me to find. Often, they’re surprised. Mostly grateful. Occasionally hostile. I can understand the discomfort. On the surface, without more in-depth knowledge, it can feel intrusive. Who asked me to scan their infrastructure? Who gave me permission to notify them about something they didn’t request? I’ve come to realize that in cybersecurity, waiting for consent isn’t always an ethical luxury we can afford. Sometimes, when a vulnerability threatens publicly accessible systems, the consequences of inaction quickly outweigh the social comfort of protocol. Ethics of Vulnerability Disclosure At the Dutch Institute for Vulnerability Disclosure, I work in a team that operates in that uncomfortable space between respecting boundaries and preventing harm. It’s not a line we walk casually, and one I’ve learned to approach with both caution and conviction. That’s why, at DIVD, we rely on a shared Code of Conduct (CoC). This CoC is derived from a policy that was published by the Dutch Public Prosecutor in 2020, which describes the circumstances under which computer hacking will be considered ethical and exempt from prosecution. This policy is based on years of debate and legal jurisprudence and has served the Dutch hacker community greatly up until now. The questions it provides to determine whether or not an action is ethical are as follows: Was the action taken in the context of a significant public interest? Was the conduct proportionate (i.e., did the suspect not go further than necessary to achieve their objective)? Was the requirement of subsidiarity met (i.e., were there no less intrusive means available to achieve the intended objective)? The DIVD CoC guides our decisions to make sure we do not cross any ethical boundaries. This can get knotty quite quickly, as we attempt to locate everyone worldwide that may be vulnerable to a particular security vulnerability. Therefore we have to make well-thought-through decisions when it comes to worldwide vulnerability scans: we have to be sure that we do not intrude further than necessary and that our way of scanning is the least impactful one. I’ve found myself to be increasingly exacting about this. If a scan doesn’t meet the ethical standards we’ve committed to at DIVD, it’s not uncommon to argue against executing it–even if that means walking away from a serious case. That may seem cautious, but it raises a deeper ethical question: When is a vulnerability severe enough that inaction becomes the more problematic choice? This post argues that the ethical frameworks implicitly used by DIVD, particularly threshold deontology, provide a defensible basis for more intrusive forms of unsolicited vulnerability disclosure when the public interest is at stake. When inaction becomes the problem Ethics in the computer security landscape have always been a topic of discussion. Recently, this discussion seems to have picked up in the academic world around the three leading frameworks and their respective ideologies: Consequentialism: Actions are morally right if it leads to the best overall outcomes or consequences. The use of consequentialism in this article mostly resembles utilitarianism, which is a type of consequentialism that focuses on the well-being of people. Deontology: Actions are morally right if it follows a set of moral rules or duties, regardless of the outcome. Virtue Ethics: Actions are morally right if it reflects the character and virtues of a good or morally exemplary person. While equally important, this post does not focus on virtue ethics. The challenge we face in Coordinated Vulnerability Disclosure is not about judging moral character. It’s about operationalizing structured processes, making ethically defensible decisions under pressure, and balancing duties with consequences. These are domains where rules and outcomes matter more directly than personal virtue. This is confirmed by various studies in the academic landscape such as The Menlo Report and the (more recent) study on Computer Security Trolley Problems by Kohno et al. These studies emphasize the importance of consequentialism and deontology where they intentionally leave virtue ethics out of scope. In contrast, studies of cybercrime — where intent, personal responsibility, and moral development are central — often lean more heavily on virtue ethics. The tension between doing what’s right according to principle and doing what’s necessary to prevent harm lies at the center of many dilemmas in computer security. Deontology and consequentialism are often seen as opposites that lead to different outcomes when applied to the same case studies, leading these case studies to be seen as moral dilemmas. For this reason, one may realise that an absolutist approach to either of these frameworks may not be sufficient in practice when the intent is preventing harm. The Stanford Encyclopedia of Philosophy emphasizes this limitation by describing a balance through what is known as threshold deontology. Threshold deontology begins with a commitment to deontological principles such as minimizing intrusion and acting transparently. However, it recognizes that these rules may need to be overridden when the potential harm of inaction crosses a critical threshold. In other words: We follow the rules, until not following them becomes the more ethical choice. Threshold deontology doesn’t abandon principles. It does, however, ask us to honor them until the consequences of strict adherence become morally unacceptable, to then act with caution for a societal cause. Principles in practice But how would we know when that threshold is actually crossed? Threshold deontology provides us with the ‘philosophical permission’ to override a duty, but it doesn’t say when exactly that override is justified. This is where the principlist framework can provide some guidance. Instead of relying on a single guiding rule, it asks us to weigh multiple ethical principles that often come into tension in practice. This helps us to assess not just whether an action is justified, but also why and what ethical trade-offs we are accepting in the process. In 2021, Formosa et al. proposed a principlist framework for cybersecurity that is composed of the following five principles: Beneficence: Promote well-being and prevent harm Non-maleficence: Avoid causing harm Autonomy: Respect individuals’ control over their systems and data Justice: Ensure fairness and equitable treatment Explicability: Act transparently and be accountable Formosa’s principlist framework is derived from Beauchamp and Childress’s “Four Principles” of biomedical ethics and added a fifth principle of explicability, which is drawn from AI ethics (Floridi et al., 2018). When we’re considering something like a global vulnerability scan, these principles help us structure our ethical reasoning. DIVD’s scanning decisions are centered on the principle of beneficence: the obligation to prevent harm and promote public safety. When the potential benefit of scanning is low, for example when a vulnerability poses little risk or is unlikely to be exploited, we hold firm to the non-maleficence (avoiding harm) and autonomy (respecting consent and responsibility) principles. In such cases, we will refrain from scanning because the ethical cost outweighs the limited benefit. However, when the potential to prevent significant harm is high, such as when a vulnerability threatens large-scale exploitation or critical infrastructure, we may override non-maleficence and autonomy in service of that benefit. This is not a decision taken lightly. It reflects a careful ethical trade-off, where the duty to protect others justifies limited, well-controlled intrusion. How WIDE is my fingerprint? To teach others about fingerprinting ethics, I often use a simple heuristic to assess the moral footprint of a fingerprinting technique called WIDE. WIDE stands for: Weaponized: Could this scanning methodology be used to (enable) harm? Or more practical: if the scanning methodology involves a public Proof of Concept, does it contain any malware that we would need to neutralize first? This reflects the principle of non-maleficence. Intrusive: Does this scanning methodology cross any boundaries of consent, privacy, or proportionality? Does it leave any unnecessary traces on the target system? This brings autonomy and justice into view. Deweaponized: Is this scanning methodology deliberately designed to reduce its exploitability? This ties to beneficence, the duty to protect. Ethical: Would this technique hold up under scrutiny from others? Is it proportionate and according to subsidiarity standards? Here, explicability becomes essential to ensure transparency on decisions and reasoning. When I ask myself, “How WIDE is this fingerprint?”, I’m not answering a closed question. I’m surfacing tensions. Even techniques that appear technically harmless can become ethically problematic if used carelessly, at scale, or without transparency. WIDE isn’t a substitute for principlism, but it does help bring the principles into everyday practice. It’s a kind of ethical gut check for ethical proportionality: quick, imperfect, but useful when decisions happen fast. Of course, ethics are subjective, which is why not everyone may agree with our framing. Especially when consent is missing, ethical objections matter. Addressing ethical objections There are some counterarguments to unsolicited scanning, such as concerns about overreach, digital trespassing, and the potential erosion of trust in security research. In the end, if subjectivity leads to the justification of compromising on the explicability principle, ethical boundaries are allowed to shift under pressure. How do we ensure they don’t shift too far? These are legitimate concerns. However, this is exactly why it is important to rely on structured frameworks like threshold deontology and principlism: not to escape ethical boundaries, but to make these boundaries visible, contestable, and constrained. The point isn’t that anything is allowed when societal safety is at play. It’s that sometimes, doing nothing carries a greater ethical cost than acting carefully without permission. The public interest threshold of Log4Shell When Log4Shell was disclosed in late 2021, it posed a severe threat to global digital infrastructure. The vulnerability affected countless systems, many of which were unknown to be using this software, and exploitation began within hours of public disclosure as it was trivial to exploit. It became clear quickly that this vulnerability didn’t just pose a theoretical risk, but a real-world crisis. Log4J, the vulnerable software, was a logging component embedded in many other systems, making this a supply chain issue. The team at DIVD working on this case then faced a difficult question: how to responsibly notify affected organizations across the globe, many of which had no idea they were even using Log4J? In such a high-stakes context, the threshold for ethical intervention was clearly crossed. The potential for harm was immense, as this vulnerability could lead to ransomware, data breaches, and critical infrastructure failures, disrupting society on a large scale. Because Log4J was embedded in other software rather than a standalone component, scanning was not possible without triggering the vulnerability itself. The considerations central to the WIDE heuristic helped to assess this approach: where many actors were mass-exploiting systems at random to test for the vulnerability, DIVD’s scanning methodology was deliberately deweaponized to reduce exploitability and kept as non-intrusive as possible by avoiding persistence or harmful effects. To achieve this, DIVD created a Log4Shell exploit that triggered a single DNS request to a Canary Token from inside the vulnerable system: an approach that was harmless and, of all options, the least intrusive. It only revealed the vulnerable IP address and nothing more. The actions taken were designed to be ethical: proportionate to the risk, grounded in subsidiarity, and subject to public scrutiny. In doing so, we aimed to maximize beneficence, minimize maleficence, respect justice, and uphold explicability—even where autonomy had to be limited for the sake of public safety. Conclusion: Trust through transparent ethics Coordinated Vulnerability Disclosure isn’t just a technical challenge: it’s an ethical one. At DIVD, we don’t treat unsolicited scanning and disclosure as a loophole or an afterthought. We treat it as an action that requires justification, restraint, and transparency. Threshold deontology provides us with the ethical architecture to act decisively when society is at risk. The principlist framework implicitly helps us navigate that threshold with clarity, so we’re not acting on instinct, but on structured ethical reasoning. I’m sharing this to explain how I think about the ethics behind unsolicited disclosure. Not because we see ourselves as above ethical rules at DIVD, but because we try to follow them as rigorously as possible, even when the path forward isn’t obvious.]]></summary></entry></feed>