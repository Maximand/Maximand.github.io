---
layout: article
title: "Unsolicited but Ethical: Threshold Deontology in Public Interest Vulnerability Disclosure"
tags:
  - Cybersecurity Policy & Ethics
  - Vulnerability Disclosure
mathjax: true
author: Max van der Horst
show_author_profile: true
show_edit_on_github: false
header:
  theme: dark
article_header:
  type: cover
  image:
    src: /assets/ethics-cvd.png
---

I often send emails to people I've never met, about systems they didn't know were vulnerable, warning them about risks they never asked me to find. Often, they're surprised. Mostly grateful. Occasionally hostile. 

I can understand the discomfort. On the surface, without more in-depth knowledge, it can feel intrusive. Who asked us to scan their infrastructure? Who gave us permission to notify them about something they didn't request? But in cybersecurity, waiting for consent isn't always an ethical luxury we can afford. Sometimes, when a vulnerability threatens publicly accessible systems, the consequences of inaction quickly outweigh the social comfort of protocol. 

# Ethics of Vulnerability Disclosure
At the [Dutch Institute for Vulnerability Disclosure](https://divd.nl), we operate in that uncomfortable space between respecting boundaries and preventing harm. We do so not carelessly, but guided by a [Code of Conduct](https://divd.nl/code) (CoC). This CoC is derived from a [policy](https://www.om.nl/documenten/richtlijnen/2020/december/14/om-beleidsbrief-ethisch-hacken#:~:text=Het%20Openbaar%20Ministerie%20heeft%20in,tegen%20een%20ethische%20hacker%20binnenkomt.) that was published by the Dutch Public Prosecutor in 2020, which describes the circumstances under which computer hacking will be considered ethical and exempt from prosecution. This policy is made up of years of debate and legal jurisprudence and has served the Dutch hacker community greatly up until now. The questions it provides to determine whether or not an action is ethical are as follows:

* Was the action taken in the context of a significant public interest?
* Was the conduct proportionate (i.e., did the suspect not go further than necessary to achieve their objective)?
* Was the requirement of subsidiarity met (i.e., were there no less intrusive means available to achieve the intended objective)?

The DIVD CoC guides investigations to make sure we do not cross any ethical boundaries. This can get knotty quite quickly, as we attempt to locate everyone worldwide that may be vulnerable to a particular security vulnerability. Therefore we have to make well thought-through decisions when it comes to worldwide vulnerability scans: we have to be sure that we do not intrude further than necessary and that our way of scanning is the least impactful one. 

We've become quite exacting in this. If we can't meet the standards in our CoC, we'll usually choose not to scan at all. That may seem cautious, but it raises a deeper ethical question: When is a vulnerability severe enough that inaction becomes the more problematic choice?

This post argues that the ethical frameworks implicitly used by DIVD, particularly threshold deontology, provide a defensible basis for more intrusive forms of unsolicited vulnerability disclosure when the public interest is at stake.

# When inaction becomes the problem
Ethics in the computer security landscape have always been a topic of discussion. Recently, this discussion seems to have picked up in the academic world around the three leading frameworks and their respective ideologies:

* **Consequentialism:** Actions are morally right if it leads to the best overall outcomes or consequences. The use of consequentialism in this article mostly resembles utilitarianism, which is a type of consequentialism that focuses on the well-being of people.
* **Deontology:** Actions are morally right if it follows a set of moral rules or duties, regardless of the outcome.
* **Virtue Ethics:** Actions are morally right if it reflects the character and virtues of a good or morally exemplary person.

While equally important, this post does not focus on virtue ethics. The challenge we face in Coordinated Vulnerability Disclosure is not about judging moral character. It’s about operationalizing structured processes, making ethically defensible decisions under pressure, and balancing duties with consequences. These are domains where rules and outcomes matter more directly than personal virtue.

This is confirmed by various studies in the academic landscape such as [The Menlo Report](https://www.dhs.gov/sites/default/files/publications/CSD-MenloPrinciplesCORE-20120803_1.pdf) and the (more recent) study on [Computer Security Trolley Problems](https://www.usenix.org/system/files/usenixsecurity23-kohno.pdf) by Kohno et al. These studies emphasize the importance of consequentialism and deontology where they intentionally leave virtue ethics out of scope. In contrast, studies of cybercrime — where intent, personal responsibility, and moral development are central — often lean more heavily on virtue ethics.

The tension between doing what's right according to principle and doing what's necessary to prevent harm lies at the center of many dilemmas in computer security. Deontology and consequentialism are often seen as opposites that lead to different outcomes when applied to the same case studies, leading these case studies to be seen as moral dilemmas. For this reason, one may realise that an absolutist approach to either of these frameworks may not be sufficient in practice when the intent is preventing harm. The Stanford Encyclopedia of Philosophy emphasizes this limitation by describing a balance through what is known as [threshold deontology](https://plato.stanford.edu/entries/ethics-deontological/#DeoRelConRec). Threshold deontology begins with a commitment to deontological principles such as minimizing intrusion and acting transparently. However, it recognizes that these rules may need to be overridden when the potential harm of inaction crosses a critical threshold. In order words:

> We follow the rules, until not following them becomes the more ethical choice.

Threshold deontology doesn't abandon principles. It does, however, ask us to honor them until the consequences of strict adherence become morally unacceptable, to then act with caution for a societal cause.

# Principles in practice
But how would we know when that threshold is actually crossed? Threshold deontology provides us with the 'philosophical permission' to override a duty, but it doesn't say when exactly that override is justified. This is where the principlist framework can provide some guidance. Instead of relying on a single guiding rule, it asks to weigh multiple ethical principles that often come into tension in practice. This helps us to assess not just whether an action is justified, but also why and what ethical trade-offs we are accepting in the process.

In 2021, Formosa et al. proposed [a principlist framework](https://www.sciencedirect.com/science/article/pii/S0167404821002066) for cybersecurity that is composed of the following five principles:

* **Beneficence:** Promote well-being and prevent harm
* **Non-maleficence**: Avoid causing harm
* **Autonomy:** Respect individuals' control over their systems and data
* **Justice:** Ensure fairness and equitable treatment
* **Explicability:** Act transparently and be accountable

Formosa's principlist framework is derived from Beauchamp and Childress's ["Four Principles" of biomedical ethics](https://jme.bmj.com/content/28/5/332.2) and added a fifth principle of explicability, which is drawn from [AI ethics](https://link.springer.com/article/10.1007/s11023-018-9482-5) (Floridi et al., 2018). When we're considering something like a global vulnerability scan, these principles help us structure our ethical reasoning. DIVD's scanning decisions are centered on the principle of beneficence: the obligation to prevent harm and promote public safety. When the potential benefit of scanning is low, for example when a vulnerability poses little risk or is unlikely to be exploited, we hold firm to the non-maleficence (avoiding harm) and autonomy (respecting consent) principles. In such cases, we will refrain from scanning because the ethical cost outweighs the limited benefit. 

However, when the potential to prevent significant harm is high, such as when a vulnerability threatens large-scale exploitation or critical infrastructure, we may override non-maleficence and autonomy in service of that benefit. This is not a decision taken lightly. It reflects a careful ethical trade-off, where the duty to protect others justifies limited, well-controlled intrusion. Of course, ethics are subjective. That is why not everyone will agree that acting without consent, even if done so in the name of public interest, can be ethically justified. 

# Addressing ethical objections
There are some [counterarguments to unsolicited scanning](https://www.hup.harvard.edu/books/9780674976009), such as concerns about overreach, digital trespassing, and the potential erosion of trust in security research. In the end, if subjectivity leads to the justification of compromising on the explicability principle, ethical boundaries are allowed to shift under pressure. How do we ensure they don't shift too far?

These are legitimate concerns. However, this is exactly why it is important to rely on structured frameworks like threshold deontology and principlism: not to escape ethical boundaries, but to make these boundaries visible, contestable, and constrained. The point isn't that anything is allowed when societal safety is at play. It's that sometimes, doing nothing carries a greater ethical cost than acting carefully without permission.

# The public interest threshold of Log4Shell
When [Log4Shell](https://www.cisa.gov/news-events/news/apache-log4j-vulnerability-guidance) was disclosed in late 2021, it posed a severe threat to global digital infrastructure. The vulnerability affected countless systems, many of which were unknown to be using this software, and exploitation began within hours of public disclosure as it was trivial to exploit. It became clear quickly that this vulnerability did not only yield a theoretical risk, but a real-world crisis.

Log4J, the vulnerable software, was a logging component embedded in a lot of other software, making this a supply chain issue. The team at DIVD working on this case then faced a difficult question: how to responsibly notify affected organizations across the globe, many of which had no idea they were even using Log4J? 

In such a high-stakes context, the threshold for ethical intervention was clearly crossed. The potential for harm was great, as this vulnerability could lead to ransomware, data breaches, and critical infrastructure failures, essentially disrupting society on a great scale. Because Log4J was embedded in other software rather than a standalone component, scanning was not possible without exploiting the vulnerability. Scanning methods were designed to be as minimally invasive as possible (non-maleficence), applied to all IP addresses in the IPv4 space (justice), and [published](https://www.divd.nl/newsroom/articles/case-apache-log4j2/) our approach (explicability). It is true that this approach compromised the autonomy of system owners. However, through careful considerations of various principles that helped justify the ethical trade-off: the duty to societal safety outweighed the duty to be non-intrusive.


# Conclusion: Trust through transparent ethics
Coordinated Vulnerability Disclosure isn't just a technical challenge: it's an ethical one. At DIVD, we don't treat unsolicited scanning and disclosure as a loophole or an afterthought. We treat it as an action that requires justification, restraint, and transparency. 

Threshold deontology provides us with the ethical architecture to act decisively when society is at risk. The principlist framework implicitly helps us navigate that threshold with clarity, so we're not acting on instinct, but on structured ethical reasoning. This post outlines the thinking that shapes this reasoning. Not claim an ethical exception, but to show how responsibility and action can go hand in hand.

